{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENui8tZN4Wlv",
        "outputId": "9737b3f1-4116-4fc7-fe55-8c8281f4715e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking for NLTK packages...\n",
            "Downloading necessary NLTK packages...\n",
            "NLTK packages downloaded successfully.\n",
            "Preprocessing sentence 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing sentence 2...\n",
            "MRPC dataset loaded successfully.\n",
            "\n",
            "Training data shape: (3260, 21272)\n",
            "Testing data shape: (816, 21272)\n",
            "\n",
            "Training the model...\n",
            "Model training complete.\n",
            "\n",
            "Model Accuracy: 0.7022\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.19      0.29       265\n",
            "           1       0.71      0.95      0.81       551\n",
            "\n",
            "    accuracy                           0.70       816\n",
            "   macro avg       0.67      0.57      0.55       816\n",
            "weighted avg       0.69      0.70      0.64       816\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 50 215]\n",
            " [ 28 523]]\n",
            "\n",
            "--- Testing with new sentences ---\n",
            "Sentence 1: 'The government has announced new tax policies.'\n",
            "Sentence 2: 'New taxation policies were revealed by the government.'\n",
            "Result: Paraphrase (Confidence: 0.81)\n",
            "\n",
            "\n",
            "Sentence 1: 'The birds are singing in the trees.'\n",
            "Sentence 2: 'I need to buy groceries from the store.'\n",
            "Result: Paraphrase (Confidence: 0.60)\n"
          ]
        }
      ],
      "source": [
        "# Paraphrase Identification using Scikit-Learn\n",
        "# This script demonstrates a basic implementation of the paraphrase identification task.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "# --- Download NLTK data (only needs to be done once) ---\n",
        "# The NLTK downloader will check if the packages are already present and up-to-date.\n",
        "try:\n",
        "    print(\"Checking for NLTK packages...\")\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('tokenizers/punkt_tab') # Check for punkt_tab as well\n",
        "    print(\"NLTK packages are already up-to-date.\")\n",
        "except LookupError:\n",
        "    print(\"Downloading necessary NLTK packages...\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('punkt_tab') # Download punkt_tab\n",
        "    print(\"NLTK packages downloaded successfully.\")\n",
        "\n",
        "\n",
        "# --- 1. Data Loading and Preprocessing ---\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses a single sentence:\n",
        "    - Tokenizes\n",
        "    - Converts to lowercase\n",
        "    - Removes punctuation\n",
        "    - Removes stopwords\n",
        "    - Lemmatizes\n",
        "    \"\"\"\n",
        "    # Ensure text is a string\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def load_and_preprocess_data(filepath):\n",
        "    \"\"\"Loads the MRPC dataset and applies preprocessing.\"\"\"\n",
        "    # The 'on_bad_lines' parameter is used to skip rows that have parsing errors.\n",
        "    df = pd.read_csv(filepath, sep='\\t', on_bad_lines='skip', quoting=3) # quoting=3 ignores quotes\n",
        "\n",
        "    # Ensure the correct columns are being selected after loading\n",
        "    # The MRPC dataset format is: Quality #1 ID #2 ID #1 String #2 String\n",
        "    df = df.iloc[:, [0, 3, 4]] # Select columns by index\n",
        "    df.columns = ['label', 'sentence1', 'sentence2']\n",
        "\n",
        "    print(\"Preprocessing sentence 1...\")\n",
        "    df['sentence1_processed'] = df['sentence1'].astype(str).apply(preprocess_text)\n",
        "    print(\"Preprocessing sentence 2...\")\n",
        "    df['sentence2_processed'] = df['sentence2'].astype(str).apply(preprocess_text)\n",
        "\n",
        "    return df\n",
        "\n",
        "# A common place to find it is: https://www.microsoft.com/en-us/download/details.aspx?id=52398\n",
        "\n",
        "try:\n",
        "    # Try loading the real dataset\n",
        "    data_path = '/content/msr_paraphrase_train.txt'\n",
        "    df = load_and_preprocess_data(data_path)\n",
        "    print(\"MRPC dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"MRPC dataset not found. Creating a dummy dataset for demonstration.\")\n",
        "    dummy_data = {\n",
        "        'label': [1, 0, 1, 0, 1],\n",
        "        'sentence1': [\n",
        "            \"The cat sat on the mat.\",\n",
        "            \"The dog played in the park.\",\n",
        "            \"What is the weather like today?\",\n",
        "            \"I love to eat pizza.\",\n",
        "            \"The company is located in New York.\"\n",
        "        ],\n",
        "        'sentence2': [\n",
        "            \"On the mat, the cat sat.\",\n",
        "            \"The sun is shining brightly.\",\n",
        "            \"How is the weather today?\",\n",
        "            \"I enjoy eating pasta.\",\n",
        "            \"The firm's headquarters are in New York City.\"\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(dummy_data)\n",
        "    df['sentence1_processed'] = df['sentence1'].apply(preprocess_text)\n",
        "    df['sentence2_processed'] = df['sentence2'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# --- 2. Feature Engineering ---\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "# We will fit it on both sentence columns to build a comprehensive vocabulary.\n",
        "corpus = pd.concat([df['sentence1_processed'], df['sentence2_processed']]).unique()\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# Transform the sentences into TF-IDF vectors\n",
        "X1 = vectorizer.transform(df['sentence1_processed'])\n",
        "X2 = vectorizer.transform(df['sentence2_processed'])\n",
        "\n",
        "# Combine the features for each sentence pair\n",
        "# We'll simply concatenate the vectors\n",
        "X = np.hstack((X1.toarray(), X2.toarray()))\n",
        "y = df['label']\n",
        "\n",
        "\n",
        "# --- 3. Model Training ---\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if len(y.unique()) > 1 else None)\n",
        "\n",
        "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "print(\"\\nTraining the model...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "\n",
        "# --- 4. Evaluation ---\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Display a detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "# Display the confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "# --- 5. Example Prediction ---\n",
        "\n",
        "def predict_paraphrase(sentence1, sentence2):\n",
        "    \"\"\"\n",
        "    Takes two sentences and predicts if they are a paraphrase.\n",
        "    \"\"\"\n",
        "    # Preprocess the input sentences\n",
        "    s1_processed = preprocess_text(sentence1)\n",
        "    s2_processed = preprocess_text(sentence2)\n",
        "\n",
        "    # Vectorize the processed sentences\n",
        "    v1 = vectorizer.transform([s1_processed])\n",
        "    v2 = vectorizer.transform([s2_processed])\n",
        "\n",
        "    # Combine the vectors\n",
        "    combined_vector = np.hstack((v1.toarray(), v2.toarray()))\n",
        "\n",
        "    # Make a prediction\n",
        "    prediction = model.predict(combined_vector)\n",
        "    probability = model.predict_proba(combined_vector)\n",
        "\n",
        "    if prediction[0] == 1:\n",
        "        return f\"Result: Paraphrase (Confidence: {probability[0][1]:.2f})\"\n",
        "    else:\n",
        "        return f\"Result: Not a Paraphrase (Confidence: {probability[0][0]:.2f})\"\n",
        "\n",
        "# Example Usage\n",
        "print(\"\\n--- Testing with new sentences ---\")\n",
        "test_sentence_1 = \"The government has announced new tax policies.\"\n",
        "test_sentence_2 = \"New taxation policies were revealed by the government.\"\n",
        "print(f\"Sentence 1: '{test_sentence_1}'\")\n",
        "print(f\"Sentence 2: '{test_sentence_2}'\")\n",
        "print(predict_paraphrase(test_sentence_1, test_sentence_2))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "test_sentence_3 = \"The birds are singing in the trees.\"\n",
        "test_sentence_4 = \"I need to buy groceries from the store.\"\n",
        "print(f\"Sentence 1: '{test_sentence_3}'\")\n",
        "print(f\"Sentence 2: '{test_sentence_4}'\")\n",
        "print(predict_paraphrase(test_sentence_3, test_sentence_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0NDdcRCW5YY_",
        "outputId": "f545cd66-e179-4c0c-a3c0-9befd61f039d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'glove.6B.100d.txt' not found.\n",
            "Downloading GloVe embeddings from http://nlp.stanford.edu/data/glove.6B.zip...\n",
            "Download complete.\n",
            "Unzipping 'glove.6B.zip'...\n",
            "Successfully unzipped. Extracted 'glove.6B.100d.txt'.\n",
            "Loading GloVe Embeddings...\n",
            "Loaded 400000 word vectors.\n",
            "MRPC dataset not found. Using a dummy dataset for demonstration.\n",
            "Found 35 unique tokens.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,600</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                     │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │      \u001b[38;5;34m3,600\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m42,240\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ embedding[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                     │                   │            │ lstm[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">45,840</span> (179.06 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m45,840\u001b[0m (179.06 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> (165.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m42,240\u001b[0m (165.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,600</span> (14.06 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,600\u001b[0m (14.06 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training the Siamese LSTM model...\n",
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.2500 - loss: 2.6187 - val_accuracy: 1.0000 - val_loss: 0.0023\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2500 - loss: 2.1874 - val_accuracy: 1.0000 - val_loss: 0.0024\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.2500 - loss: 1.8065 - val_accuracy: 1.0000 - val_loss: 0.0026\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.2500 - loss: 1.4967 - val_accuracy: 1.0000 - val_loss: 0.0027\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.2500 - loss: 1.2607 - val_accuracy: 1.0000 - val_loss: 0.0028\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.2500 - loss: 1.0753 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.2500 - loss: 0.9236 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.2500 - loss: 0.7882 - val_accuracy: 1.0000 - val_loss: 0.0030\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2500 - loss: 0.6786 - val_accuracy: 1.0000 - val_loss: 0.0031\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.5000 - loss: 0.5863 - val_accuracy: 1.0000 - val_loss: 0.0032\n",
            "Model training complete.\n",
            "\n",
            "Evaluating the model on the validation set...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step\n",
            "\n",
            "Model Accuracy: 1.0000\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Paraphrase Identification using a Siamese LSTM Network with GloVe Embeddings\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# --- TensorFlow and Keras Imports ---\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --- Download NLTK data ---\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    print(\"Downloading necessary NLTK packages...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    print(\"NLTK packages are ready.\")\n",
        "\n",
        "# --- 1. Download and Load GloVe Word Embeddings ---\n",
        "def download_and_load_glove(glove_file=\"glove.6B.100d.txt\", zip_file=\"glove.6B.zip\", url=\"http://nlp.stanford.edu/data/glove.6B.zip\"):\n",
        "    \"\"\"\n",
        "    Downloads, unzips, and loads GloVe embeddings if they don't exist.\n",
        "    \"\"\"\n",
        "    # Check if the GloVe text file already exists\n",
        "    if not os.path.exists(glove_file):\n",
        "        print(f\"'{glove_file}' not found.\")\n",
        "        # Check if the zip file exists\n",
        "        if not os.path.exists(zip_file):\n",
        "            print(f\"Downloading GloVe embeddings from {url}...\")\n",
        "            try:\n",
        "                response = requests.get(url, stream=True)\n",
        "                response.raise_for_status() # Raise an exception for bad status codes\n",
        "                with open(zip_file, \"wb\") as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                print(\"Download complete.\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading file: {e}\")\n",
        "                return None\n",
        "\n",
        "        # Unzip the file\n",
        "        print(f\"Unzipping '{zip_file}'...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "                zip_ref.extractall()\n",
        "            print(f\"Successfully unzipped. Extracted '{glove_file}'.\")\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"Error: '{zip_file}' is not a valid zip file. Please delete it and try again.\")\n",
        "            return None\n",
        "\n",
        "    # Load the embeddings from the text file\n",
        "    print(\"Loading GloVe Embeddings...\")\n",
        "    embeddings_dict = {}\n",
        "    try:\n",
        "        with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], \"float32\")\n",
        "                embeddings_dict[word] = vector\n",
        "        print(f\"Loaded {len(embeddings_dict)} word vectors.\")\n",
        "        return embeddings_dict\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Critical Error: GloVe file '{glove_file}' could not be found after download/unzip attempt.\")\n",
        "        return None\n",
        "\n",
        "glove_embeddings = download_and_load_glove()\n",
        "\n",
        "# --- 2. Data Loading and Preprocessing ---\n",
        "def load_data(filepath):\n",
        "    \"\"\"Loads and cleans the MRPC dataset.\"\"\"\n",
        "    df = pd.read_csv(filepath, sep='\\t', on_bad_lines='skip', quoting=3)\n",
        "    df = df.iloc[:, [0, 3, 4]]\n",
        "    df.columns = ['label', 'sentence1', 'sentence2']\n",
        "    # Drop rows with missing values\n",
        "    df.dropna(inplace=True)\n",
        "    return df\n",
        "\n",
        "try:\n",
        "    data_path = 'msr-paraphrase-train.txt'\n",
        "    df = load_data(data_path)\n",
        "    print(\"MRPC dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"MRPC dataset not found. Using a dummy dataset for demonstration.\")\n",
        "    dummy_data = {\n",
        "        'label': [1, 0, 1, 0, 1],\n",
        "        'sentence1': [\"The cat sat on the mat.\", \"The dog played in the park.\", \"What is the weather like today?\", \"I love to eat pizza.\", \"The company is located in New York.\"],\n",
        "        'sentence2': [\"On the mat, the cat sat.\", \"The sun is shining brightly.\", \"How is the weather today?\", \"I enjoy eating pasta.\", \"The firm's headquarters are in New York City.\"]\n",
        "    }\n",
        "    df = pd.DataFrame(dummy_data)\n",
        "\n",
        "# --- 3. Data Preparation for Keras ---\n",
        "if glove_embeddings:\n",
        "    # Combine all sentences for tokenizer vocabulary\n",
        "    all_sentences = pd.concat([df['sentence1'], df['sentence2']]).astype(str)\n",
        "\n",
        "    # Initialize and fit tokenizer\n",
        "    tokenizer = Tokenizer(num_words=10000, oov_token='<unk>')\n",
        "    tokenizer.fit_on_texts(all_sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "    print(f\"Found {len(word_index)} unique tokens.\")\n",
        "\n",
        "    # Convert sentences to sequences of integers\n",
        "    seq1 = tokenizer.texts_to_sequences(df['sentence1'].astype(str))\n",
        "    seq2 = tokenizer.texts_to_sequences(df['sentence2'].astype(str))\n",
        "\n",
        "    # Pad sequences to a max length\n",
        "    MAX_SEQUENCE_LENGTH = 30\n",
        "    data1 = pad_sequences(seq1, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    data2 = pad_sequences(seq2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    labels = df['label'].values\n",
        "\n",
        "    # --- 4. Create GloVe Embedding Matrix ---\n",
        "    EMBEDDING_DIM = 100 # Must match the GloVe file dimension\n",
        "    num_words = min(10000, len(word_index) + 1)\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i >= num_words:\n",
        "            continue\n",
        "        embedding_vector = glove_embeddings.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    # --- 5. Build the Siamese LSTM Model ---\n",
        "\n",
        "    # Define the shared layers\n",
        "    embedding_layer = Embedding(\n",
        "        num_words,\n",
        "        EMBEDDING_DIM,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=MAX_SEQUENCE_LENGTH,\n",
        "        trainable=False # We don't want to train the pre-trained GloVe embeddings\n",
        "    )\n",
        "\n",
        "    lstm_layer = LSTM(64) # 64 is the number of LSTM units\n",
        "\n",
        "    # Define the model inputs\n",
        "    input1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "    input2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "\n",
        "    # Define the two towers of the Siamese network\n",
        "    tower1 = embedding_layer(input1)\n",
        "    tower1 = lstm_layer(tower1)\n",
        "\n",
        "    tower2 = embedding_layer(input2)\n",
        "    tower2 = lstm_layer(tower2)\n",
        "\n",
        "    # Define the distance function (Manhattan distance)\n",
        "    def manhattan_distance(vectors):\n",
        "        vec1, vec2 = vectors\n",
        "        return K.exp(-K.sum(K.abs(vec1 - vec2), axis=1, keepdims=True))\n",
        "\n",
        "    distance = Lambda(manhattan_distance)([tower1, tower2])\n",
        "\n",
        "    # The final model\n",
        "    model = Model(inputs=[input1, input2], outputs=distance)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # --- 6. Train the Model ---\n",
        "\n",
        "    # Split data\n",
        "    indices = np.arange(data1.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    data1 = data1[indices]\n",
        "    data2 = data2[indices]\n",
        "    labels = labels[indices]\n",
        "\n",
        "    num_validation_samples = int(0.2 * data1.shape[0])\n",
        "\n",
        "    x1_train = data1[:-num_validation_samples]\n",
        "    x2_train = data2[:-num_validation_samples]\n",
        "    y_train = labels[:-num_validation_samples]\n",
        "\n",
        "    x1_val = data1[-num_validation_samples:]\n",
        "    x2_val = data2[-num_validation_samples:]\n",
        "    y_val = labels[-num_validation_samples:]\n",
        "\n",
        "    print(\"\\nTraining the Siamese LSTM model...\")\n",
        "    history = model.fit(\n",
        "        [x1_train, x2_train], y_train,\n",
        "        validation_data=([x1_val, x2_val], y_val),\n",
        "        epochs=10,\n",
        "        batch_size=64\n",
        "    )\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    # --- 7. Evaluation ---\n",
        "\n",
        "    print(\"\\nEvaluating the model on the validation set...\")\n",
        "    # Predict probabilities\n",
        "    y_pred_probs = model.predict([x1_val, x2_val])\n",
        "    # Convert probabilities to binary predictions (0 or 1)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_val, y_pred, zero_division=0))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_val, y_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gr3mbo3x9Y0-",
        "outputId": "f3114817-7e9f-4a80-e555-cea9c8278af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GloVe Embeddings...\n",
            "Loaded 400000 word vectors.\n",
            "Successfully loaded /content/msr_paraphrase_test.txt\n",
            "Successfully loaded /content/msr_paraphrase_test.txt\n",
            "Found 8842 unique tokens in training data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">884,300</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │                   │            │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                     │                   │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │    \u001b[38;5;34m884,300\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │ input_layer_5[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m42,240\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │                   │            │ embedding_2[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                     │                   │            │ lstm_2[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">926,540</span> (3.53 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m926,540\u001b[0m (3.53 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> (165.00 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m42,240\u001b[0m (165.00 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">884,300</span> (3.37 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m884,300\u001b[0m (3.37 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training the Siamese LSTM model...\n",
            "Epoch 1/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.4229 - loss: 2.3829 - val_accuracy: 0.4913 - val_loss: 1.4808\n",
            "Epoch 2/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5243 - loss: 1.2510 - val_accuracy: 0.5145 - val_loss: 1.1600\n",
            "Epoch 3/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5398 - loss: 0.9734 - val_accuracy: 0.5260 - val_loss: 1.0087\n",
            "Epoch 4/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5451 - loss: 0.8366 - val_accuracy: 0.5318 - val_loss: 0.9058\n",
            "Epoch 5/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5717 - loss: 0.7224 - val_accuracy: 0.5491 - val_loss: 0.8296\n",
            "Epoch 6/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6204 - loss: 0.6309 - val_accuracy: 0.5491 - val_loss: 0.7742\n",
            "Epoch 7/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6367 - loss: 0.6062 - val_accuracy: 0.5665 - val_loss: 0.7276\n",
            "Epoch 8/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6934 - loss: 0.5751 - val_accuracy: 0.5896 - val_loss: 0.7035\n",
            "Epoch 9/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7740 - loss: 0.5048 - val_accuracy: 0.6012 - val_loss: 0.6760\n",
            "Epoch 10/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7941 - loss: 0.4961 - val_accuracy: 0.6127 - val_loss: 0.6658\n",
            "Epoch 11/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8068 - loss: 0.4886 - val_accuracy: 0.6301 - val_loss: 0.6623\n",
            "Epoch 12/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7934 - loss: 0.4857 - val_accuracy: 0.6532 - val_loss: 0.6403\n",
            "Epoch 13/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8173 - loss: 0.4545 - val_accuracy: 0.6763 - val_loss: 0.6268\n",
            "Epoch 14/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8404 - loss: 0.4459 - val_accuracy: 0.6590 - val_loss: 0.6187\n",
            "Epoch 15/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8465 - loss: 0.4221 - val_accuracy: 0.6879 - val_loss: 0.6120\n",
            "Epoch 16/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8504 - loss: 0.4388 - val_accuracy: 0.6994 - val_loss: 0.6146\n",
            "Epoch 17/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8385 - loss: 0.4374 - val_accuracy: 0.6879 - val_loss: 0.6089\n",
            "Epoch 18/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8369 - loss: 0.4253 - val_accuracy: 0.6647 - val_loss: 0.6113\n",
            "Epoch 19/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8587 - loss: 0.4081 - val_accuracy: 0.6821 - val_loss: 0.6164\n",
            "Epoch 20/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8498 - loss: 0.4117 - val_accuracy: 0.6705 - val_loss: 0.6115\n",
            "Epoch 21/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8575 - loss: 0.4075 - val_accuracy: 0.6705 - val_loss: 0.6165\n",
            "Epoch 22/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8449 - loss: 0.4072 - val_accuracy: 0.6705 - val_loss: 0.5996\n",
            "Epoch 23/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8675 - loss: 0.3867 - val_accuracy: 0.6879 - val_loss: 0.6035\n",
            "Epoch 24/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8547 - loss: 0.3957 - val_accuracy: 0.6936 - val_loss: 0.5945\n",
            "Epoch 25/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8667 - loss: 0.3907 - val_accuracy: 0.6647 - val_loss: 0.6000\n",
            "Epoch 26/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8770 - loss: 0.3853 - val_accuracy: 0.6879 - val_loss: 0.6116\n",
            "Epoch 27/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8713 - loss: 0.3735 - val_accuracy: 0.6936 - val_loss: 0.6021\n",
            "Epoch 28/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8841 - loss: 0.3649 - val_accuracy: 0.6879 - val_loss: 0.5993\n",
            "Epoch 29/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8832 - loss: 0.3740 - val_accuracy: 0.6936 - val_loss: 0.5982\n",
            "Epoch 30/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8866 - loss: 0.3638 - val_accuracy: 0.6879 - val_loss: 0.5966\n",
            "Epoch 31/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8893 - loss: 0.3563 - val_accuracy: 0.6821 - val_loss: 0.6112\n",
            "Epoch 32/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8600 - loss: 0.3740 - val_accuracy: 0.6994 - val_loss: 0.6159\n",
            "Epoch 33/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8726 - loss: 0.3643 - val_accuracy: 0.6936 - val_loss: 0.6014\n",
            "Epoch 34/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8739 - loss: 0.3654 - val_accuracy: 0.6879 - val_loss: 0.6074\n",
            "Epoch 35/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8886 - loss: 0.3553 - val_accuracy: 0.6821 - val_loss: 0.6095\n",
            "Epoch 36/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8944 - loss: 0.3430 - val_accuracy: 0.6821 - val_loss: 0.6103\n",
            "Epoch 37/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8898 - loss: 0.3424 - val_accuracy: 0.6936 - val_loss: 0.6050\n",
            "Epoch 38/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8929 - loss: 0.3376 - val_accuracy: 0.6936 - val_loss: 0.6032\n",
            "Epoch 39/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8947 - loss: 0.3368 - val_accuracy: 0.6821 - val_loss: 0.5986\n",
            "Epoch 40/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9031 - loss: 0.3290 - val_accuracy: 0.6936 - val_loss: 0.6012\n",
            "Epoch 41/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8948 - loss: 0.3340 - val_accuracy: 0.6705 - val_loss: 0.6068\n",
            "Epoch 42/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8763 - loss: 0.3372 - val_accuracy: 0.6821 - val_loss: 0.6009\n",
            "Epoch 43/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9090 - loss: 0.3216 - val_accuracy: 0.6821 - val_loss: 0.6088\n",
            "Epoch 44/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9189 - loss: 0.3079 - val_accuracy: 0.6821 - val_loss: 0.6010\n",
            "Epoch 45/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8959 - loss: 0.3272 - val_accuracy: 0.6821 - val_loss: 0.6021\n",
            "Epoch 46/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9009 - loss: 0.3118 - val_accuracy: 0.6763 - val_loss: 0.6111\n",
            "Epoch 47/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9085 - loss: 0.3122 - val_accuracy: 0.6763 - val_loss: 0.6137\n",
            "Epoch 48/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9149 - loss: 0.3080 - val_accuracy: 0.6821 - val_loss: 0.6068\n",
            "Epoch 49/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9164 - loss: 0.3082 - val_accuracy: 0.6879 - val_loss: 0.6228\n",
            "Epoch 50/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9000 - loss: 0.3204 - val_accuracy: 0.6705 - val_loss: 0.6111\n",
            "Epoch 51/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9132 - loss: 0.3063 - val_accuracy: 0.6647 - val_loss: 0.6158\n",
            "Epoch 52/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9208 - loss: 0.2814 - val_accuracy: 0.6647 - val_loss: 0.6275\n",
            "Epoch 53/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8952 - loss: 0.3053 - val_accuracy: 0.6590 - val_loss: 0.6255\n",
            "Epoch 54/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9164 - loss: 0.2985 - val_accuracy: 0.6763 - val_loss: 0.6304\n",
            "Epoch 55/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9164 - loss: 0.2948 - val_accuracy: 0.6705 - val_loss: 0.6280\n",
            "Epoch 56/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9208 - loss: 0.2931 - val_accuracy: 0.6705 - val_loss: 0.6261\n",
            "Epoch 57/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9140 - loss: 0.2884 - val_accuracy: 0.6590 - val_loss: 0.6271\n",
            "Epoch 58/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9186 - loss: 0.2786 - val_accuracy: 0.6532 - val_loss: 0.6309\n",
            "Epoch 59/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9275 - loss: 0.2686 - val_accuracy: 0.6705 - val_loss: 0.6367\n",
            "Epoch 60/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9189 - loss: 0.2778 - val_accuracy: 0.6705 - val_loss: 0.6390\n",
            "Epoch 61/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9167 - loss: 0.2752 - val_accuracy: 0.6705 - val_loss: 0.6369\n",
            "Epoch 62/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9259 - loss: 0.2713 - val_accuracy: 0.6590 - val_loss: 0.6473\n",
            "Epoch 63/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9322 - loss: 0.2690 - val_accuracy: 0.6763 - val_loss: 0.6400\n",
            "Epoch 64/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9210 - loss: 0.2795 - val_accuracy: 0.6821 - val_loss: 0.6301\n",
            "Epoch 65/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9118 - loss: 0.2713 - val_accuracy: 0.6647 - val_loss: 0.6510\n",
            "Epoch 66/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9244 - loss: 0.2652 - val_accuracy: 0.6821 - val_loss: 0.6440\n",
            "Epoch 67/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9334 - loss: 0.2534 - val_accuracy: 0.6763 - val_loss: 0.6515\n",
            "Epoch 68/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9254 - loss: 0.2582 - val_accuracy: 0.6647 - val_loss: 0.6434\n",
            "Epoch 69/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9332 - loss: 0.2458 - val_accuracy: 0.6532 - val_loss: 0.6592\n",
            "Epoch 70/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9345 - loss: 0.2516 - val_accuracy: 0.6647 - val_loss: 0.6434\n",
            "Epoch 71/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9250 - loss: 0.2547 - val_accuracy: 0.6763 - val_loss: 0.6562\n",
            "Epoch 72/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9185 - loss: 0.2648 - val_accuracy: 0.6763 - val_loss: 0.6477\n",
            "Epoch 73/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9389 - loss: 0.2333 - val_accuracy: 0.6705 - val_loss: 0.6576\n",
            "Epoch 74/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9447 - loss: 0.2338 - val_accuracy: 0.6590 - val_loss: 0.6691\n",
            "Epoch 75/75\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9281 - loss: 0.2567 - val_accuracy: 0.6532 - val_loss: 0.6544\n",
            "Model training complete.\n",
            "\n",
            "Evaluating the model on the unseen test set...\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
            "\n",
            "Final Test Accuracy: 0.9142\n",
            "\n",
            "Final Test Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.82      0.86       578\n",
            "           1       0.91      0.96      0.94      1146\n",
            "\n",
            "    accuracy                           0.91      1724\n",
            "   macro avg       0.92      0.89      0.90      1724\n",
            "weighted avg       0.91      0.91      0.91      1724\n",
            "\n",
            "\n",
            "Final Test Confusion Matrix:\n",
            "[[ 472  106]\n",
            " [  42 1104]]\n"
          ]
        }
      ],
      "source": [
        "# Paraphrase Identification using a Siamese LSTM Network with GloVe Embeddings\n",
        "# This script builds, trains, and evaluates a deep learning model on the full MRPC dataset.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# --- TensorFlow and Keras Imports ---\n",
        "# You will need to run: pip install tensorflow requests\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --- Download NLTK data ---\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    print(\"Downloading necessary NLTK packages...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    print(\"NLTK packages are ready.\")\n",
        "\n",
        "# --- 1. Download and Load GloVe Word Embeddings ---\n",
        "def download_and_load_glove(glove_file=\"glove.6B.100d.txt\", zip_file=\"glove.6B.zip\", url=\"http://nlp.stanford.edu/data/glove.6B.zip\"):\n",
        "    \"\"\"\n",
        "    Downloads, unzips, and loads GloVe embeddings if they don't exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(glove_file):\n",
        "        print(f\"'{glove_file}' not found.\")\n",
        "        if not os.path.exists(zip_file):\n",
        "            print(f\"Downloading GloVe embeddings from {url}...\")\n",
        "            try:\n",
        "                response = requests.get(url, stream=True)\n",
        "                response.raise_for_status()\n",
        "                with open(zip_file, \"wb\") as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "                print(\"Download complete.\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading file: {e}\")\n",
        "                return None\n",
        "\n",
        "        print(f\"Unzipping '{zip_file}'...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "                zip_ref.extractall()\n",
        "            print(f\"Successfully unzipped. Extracted '{glove_file}'.\")\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"Error: '{zip_file}' is not a valid zip file. Please delete it and try again.\")\n",
        "            return None\n",
        "\n",
        "    print(\"Loading GloVe Embeddings...\")\n",
        "    embeddings_dict = {}\n",
        "    try:\n",
        "        with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:], \"float32\")\n",
        "                embeddings_dict[word] = vector\n",
        "        print(f\"Loaded {len(embeddings_dict)} word vectors.\")\n",
        "        return embeddings_dict\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Critical Error: GloVe file '{glove_file}' could not be found.\")\n",
        "        return None\n",
        "\n",
        "glove_embeddings = download_and_load_glove()\n",
        "\n",
        "# --- 2. Data Loading and Preprocessing ---\n",
        "def load_data(filepath, file_type):\n",
        "    \"\"\"Loads and cleans the MRPC dataset.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath, sep='\\t', on_bad_lines='skip', quoting=3)\n",
        "        # The test file has a different header, so we skip the first row\n",
        "        if file_type == 'test':\n",
        "            df = df.iloc[1:]\n",
        "        df = df.iloc[:, [0, 3, 4]]\n",
        "        df.columns = ['label', 'sentence1', 'sentence2']\n",
        "        df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
        "        df.dropna(inplace=True)\n",
        "        df['label'] = df['label'].astype(int)\n",
        "        print(f\"Successfully loaded {filepath}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {filepath} not found. Please ensure it's in the correct directory.\")\n",
        "        return None\n",
        "\n",
        "# Load both training and testing datasets\n",
        "train_df = load_data('/content/msr_paraphrase_test.txt', 'train')\n",
        "test_df = load_data('/content/msr_paraphrase_test.txt', 'test')\n",
        "\n",
        "# --- 3. Data Preparation for Keras ---\n",
        "if glove_embeddings is not None and train_df is not None and test_df is not None:\n",
        "    # Fit tokenizer ONLY on the training data to prevent data leakage\n",
        "    train_sentences = pd.concat([train_df['sentence1'], train_df['sentence2']]).astype(str)\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=15000, oov_token='<unk>')\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "    print(f\"Found {len(word_index)} unique tokens in training data.\")\n",
        "\n",
        "    # Prepare function to transform text to padded sequences\n",
        "    MAX_SEQUENCE_LENGTH = 35\n",
        "    def prepare_sequences(df, tokenizer):\n",
        "        seq1 = tokenizer.texts_to_sequences(df['sentence1'].astype(str))\n",
        "        seq2 = tokenizer.texts_to_sequences(df['sentence2'].astype(str))\n",
        "        data1 = pad_sequences(seq1, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "        data2 = pad_sequences(seq2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "        labels = df['label'].values\n",
        "        return data1, data2, labels\n",
        "\n",
        "    # Prepare train, validation, and test sets\n",
        "    x1_train, x2_train, y_train = prepare_sequences(train_df, tokenizer)\n",
        "    x1_test, x2_test, y_test = prepare_sequences(test_df, tokenizer)\n",
        "\n",
        "    # --- 4. Create GloVe Embedding Matrix ---\n",
        "    EMBEDDING_DIM = 100\n",
        "    num_words = min(15000, len(word_index) + 1)\n",
        "    embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "    for word, i in word_index.items():\n",
        "        if i >= num_words:\n",
        "            continue\n",
        "        embedding_vector = glove_embeddings.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    # --- 5. Build the Siamese LSTM Model ---\n",
        "    embedding_layer = Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
        "    lstm_layer = LSTM(64)\n",
        "    input1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "    input2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "    tower1 = lstm_layer(embedding_layer(input1))\n",
        "    tower2 = lstm_layer(embedding_layer(input2))\n",
        "\n",
        "    def manhattan_distance(vectors):\n",
        "        vec1, vec2 = vectors\n",
        "        return K.exp(-K.sum(K.abs(vec1 - vec2), axis=1, keepdims=True))\n",
        "\n",
        "    distance = Lambda(manhattan_distance)([tower1, tower2])\n",
        "    model = Model(inputs=[input1, input2], outputs=distance)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    # --- 6. Train the Model ---\n",
        "    print(\"\\nTraining the Siamese LSTM model...\")\n",
        "    history = model.fit(\n",
        "        [x1_train, x2_train], y_train,\n",
        "        validation_split=0.1, # Use 10% of training data for validation\n",
        "        epochs=75, # Increased epochs for a larger dataset\n",
        "        batch_size=64\n",
        "    )\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    # --- 7. Final Evaluation on the Unseen Test Set ---\n",
        "    print(\"\\nEvaluating the model on the unseen test set...\")\n",
        "    y_pred_probs = model.predict([x1_test, x2_test])\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\nFinal Test Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nFinal Test Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "    print(\"\\nFinal Test Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping model training due to missing data or embeddings.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k7hh6A-DzBf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
